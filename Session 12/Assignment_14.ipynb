{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of Assignment14_parinita_group_effort_95_epoch3_v100_ROHAN.ipynb","version":"0.3.2","provenance":[{"file_id":"1k6FplwI8a776f7VqUneRunIEm4F8rkuF","timestamp":1566794773353}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"t8vN1tWl2ZIr","colab_type":"text"},"source":["##Assignment14\n","###group effort : Yadunandan,Deepak, Kayal, Manjunath H , abhigoku10, Parinita R9\n","\n"]},{"cell_type":"markdown","metadata":{"id":"j7godUbeA6Xl","colab_type":"text"},"source":["### Import the required libraries\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kOsYqWHvQBrZ","outputId":"cf715df7-970c-4d0d-d727-e594f526adc7","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","import numpy as np\n","import time, math\n","from tqdm import tqdm_notebook as tqdm\n","\n","import tensorflow as tf\n","import tensorflow.contrib.eager as tfe\n","\n","import tensorflow.keras.backend as K\n","from tensorflow.keras import initializers\n","from tensorflow.keras.initializers import Initializer\n","from tensorflow.keras import regularizers\n","from tensorflow.keras import constraints\n","from tensorflow.keras.layers import InputSpec\n","from keras.backend.tensorflow_backend import _regular_normalize_batch_in_training\n","\n","from keras.preprocessing.image import ImageDataGenerator,  img_to_array, load_img\n","import numpy as np\n","import os\n","from keras.utils import np_utils\n","import pickle\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","import glob\n","from PIL import Image\n","#import tf.contrib.mixed_precision.LossScaleOptimizer\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ZFG80z-nshnt","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eEaGThLcoLxX"},"source":["### tfrecords for  train and test is already available from preprocessing"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yNyrg5NroKfm","outputId":"c9243aac-099c-435a-848a-303091940db3","colab":{}},"source":["!ls -la /output\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["total 455928\r\n","-rw-r--r-- 1 root root     51743 Aug 26 01:40  final_above95_colab_190sec_3Eoch_targetlr_with_cut_out_1024_RohanDN.ipynb\r\n","drwxr-xr-x 4 root root      4096 Aug 26 01:40 .\r\n","drwxr-xr-x 1 root root      4096 Aug 26 00:32 ..\r\n","-rw-r--r-- 1 root root       129 Aug 26 00:32 .floydignore\r\n","drwxr-xr-x 2 root root      4096 Aug 26 00:34 .ipynb_checkpoints\r\n","drwxr-xr-x 3 root root      4096 Aug 26 00:50 .keras\r\n","-rw-r--r-- 1 root root    217610 Aug 26 00:32 Assignment_14_DN_with_cutout_93_1-2_floyd_1.9_run.ipynb\r\n","-rw-r--r-- 1 root root    198810 Aug 26 00:32 DN_with_cutout_93%.ipynb\r\n","-rw-r--r-- 1 root root    204072 Aug 26 00:32 DN_with_cutout_93_1.ipynb\r\n","-rw-r--r-- 1 root root       173 Aug 26 01:34 MODELACCY1.txt\r\n","-rw-r--r-- 1 root root      2577 Aug 26 00:32 Untitled.rtf\r\n","-rw-r--r-- 1 root root    252576 Aug 26 00:32 Updated_run_re_floydgroup_Init16_08_RohanDN.ipynb\r\n","-rw-r--r-- 1 root root     12197 Aug 26 00:32 epoch_output_cloud.rtf\r\n","-rw-r--r-- 1 root root       479 Aug 26 00:32 floyd.yml\r\n","-rw-r--r-- 1 root root      4267 Aug 26 00:32 floyd_log.rtf\r\n","-rw-r--r-- 1 root root     43761 Aug 26 00:32 group_Init16_08_RohanDN.ipynb\r\n","-rw-r--r-- 1 root root    295935 Aug 26 00:32 re_floydgroup_Init16_08_RohanDN.ipynb\r\n","-rw-r--r-- 1 root root  46210080 Aug 26 00:37 test.tfrecords-000\r\n","-rw-r--r-- 1 root root 419317644 Aug 26 01:06 train.tfrecords-000\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KcVY1UhnQPDO","colab":{}},"source":["tf.enable_eager_execution()\n","K.set_floatx('float16')\n","K.set_epsilon(1e-4) #default is 1e-7"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4LfLeMAyQU7z","colab":{}},"source":["BATCH_SIZE = 512 #@param {type:\"integer\"}\n","MOMENTUM = 0.9 #@param {type:\"number\"}\n","LEARNING_RATE = .4 #@param {type:\"number\"}\n","WEIGHT_DECAY = 5e-4 #@param {type:\"number\"}\n","EPOCHS = 3 #@param {type:\"integer\"}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AwS9BRv_QWR0","colab":{}},"source":["def init_pytorch(shape, dtype=tf.float16, partition_info=None):\n","  fan = np.prod(shape[:-1])\n","  bound = 1 / math.sqrt(fan)\n","  return tf.random_uniform(shape, minval=-bound, maxval=bound, dtype=dtype)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"936s2vHU3jld","colab_type":"text"},"source":["### Model building blocks\n","\n","Create Convolution -> Batch Norm -> Activation layers"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tQsmtyO5GJod","colab":{}},"source":["class ConvBN(tf.keras.Model):\n","  def __init__(self, c_out , bnorm = True):\n","    super().__init__()\n","    self.bnorm = bnorm\n","    #self.depth = tf.keras.layers.DepthwiseConv2D(kernel_size=3, padding=\"SAME\", activation = \"relu\" , depthwise_regularizer =tf.initializers.glorot_normal() , use_bias=False)\n","    #self.conv = tf.keras.layers.Conv2D(filters=c_out, kernel_size=1, padding=\"SAME\", kernel_initializer=tf.initializers.glorot_normal(), use_bias=False)\n","    self.conv = tf.keras.layers.Conv2D(filters=c_out, kernel_size=3, padding=\"SAME\", kernel_initializer=tf.keras.initializers.glorot_normal(), use_bias=False)\n","    if self.bnorm:\n","      self.bn = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n","\n","  def call(self, inputs):\n","    #print(\"con\",inputs.shape)\n","    h = self.conv(inputs)\n","    if self.bnorm:\n","      h = self.bn(h) \n","    return tf.nn.relu(h)\n","    #return tf.nn.relu(self.bn(((self.conv((inputs)))))) #self.bn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XWQN8YVA3yiH","colab_type":"text"},"source":["#### Create Residual block \n","\n","main line - ConvBN -> ConvBN -> Maxpool\n","\n","residual line - ConvBN -> ConvBN -> ConvBN\n","\n","concatenate the result of main line and residual line"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5gn-NxDsGMXN","colab":{}},"source":["class ResBlk(tf.keras.Model):\n","  def __init__(self, c_out, pool, res = False):\n","    super().__init__()\n","    self.conv_bn = ConvBN(c_out)\n","    self.conv_bn2 = ConvBN(c_out)\n","    \n","    self.pool = pool\n","    self.res = res\n","    if self.res:\n","      self.res1 = ConvBN(c_out)\n","      self.res2 = ConvBN(c_out)\n","      self.res3 = ConvBN(c_out)\n","\n","\n","\n","  def call(self, inputs):\n","    h = self.pool(self.conv_bn2(self.conv_bn(inputs))) #run3 add extra conv roll back\n","    if self.res:\n","      #h = h + self.res2(self.res1(h)) # prev\n","      h = tf.keras.layers.concatenate([h,self.res3(self.res2(self.res1(h)))])\n","    return h"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WD41-HmM4iXa","colab_type":"text"},"source":["#### Build the model\n","\n","ConvBN -> ConvBN -> Residual block 1 -> Residual block 2 -> Global Avg Pool -> softmax"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xgfm8aMOGNSJ","colab":{}},"source":["class DavidNet(tf.keras.Model):\n","  def __init__(self, c=64, weight=0.125):\n","    super().__init__()\n","    pool = tf.keras.layers.MaxPooling2D()\n","    self.sp_drpt = tf.keras.layers.SpatialDropout2D(0.2)\n","    self.init_conv_bn = ConvBN(c)\n","    self.conv = ConvBN(c)\n","    self.blk1 = ResBlk(c*2, pool, res = True)\n","    self.blk2 = ResBlk(c*4, pool)\n","    self.blk3 = ResBlk(c*8, pool, res = True)\n","    self.pool = tf.keras.layers.GlobalMaxPool2D()\n","    self.linear = tf.keras.layers.Dense(10, kernel_initializer=tf.keras.initializers.glorot_normal(), use_bias=False)\n","    self.weight = weight\n","\n","  def call(self, x, y):\n","    h = self.pool((self.blk3((((self.blk1((self.init_conv_bn(x))))))))) # run2 add conv roll back self.blk2 self.sp_drpt\n","    h = self.linear(h) * self.weight\n","    #print(\"h \", h.dtype , \" : lab : \",y.dtype)\n","    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=y)\n","    loss = tf.reduce_sum(ce)\n","    correct = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(h, axis = 1), y), tf.float32)) # math is removed PBO\n","    return loss, correct"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"atWqKlekpQZb","colab":{}},"source":["\"\"\"from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","\"\"\"#/content/gdrive/My Drive/Colab Notebooks/eva/dataset/cifar10"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pYi9BxTJ5D7N","colab_type":"text"},"source":["### Load the dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4if1-Zq_omlV","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import os\n","\n","import numpy as np\n","\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras.datasets.cifar import load_batch\n","from tensorflow.python.keras.utils.data_utils import get_file\n","#from tensorflow.python.util.tf_export import keras_export\n","\n","\n","# @keras_export('keras.datasets.cifar10.load_data')\n","def load_data():\n","  \"\"\"Loads CIFAR10 dataset.\n","  Returns:\n","      Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n","  \"\"\"\n","  dirname = 'cifar-10-batches-py'\n","  origin = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n","  path = get_file(dirname, origin=origin, untar=True)\n","#   path = dirname #fix\n","  num_train_samples = 50000\n","\n","  x_train = np.empty((num_train_samples, 3, 32, 32), dtype='uint8')\n","  y_train = np.empty((num_train_samples,), dtype='uint8')\n","\n","  for i in range(1, 6):\n","    fpath = os.path.join(path, 'data_batch_' + str(i))\n","    (x_train[(i - 1) * 10000:i * 10000, :, :, :],\n","     y_train[(i - 1) * 10000:i * 10000]) = load_batch(fpath)\n","\n","  fpath = os.path.join(path, 'test_batch')\n","  x_test, y_test = load_batch(fpath)\n","\n","  y_train = np.reshape(y_train, (len(y_train), 1))\n","  y_test = np.reshape(y_test, (len(y_test), 1))\n","\n","  if K.image_data_format() == 'channels_last':\n","    x_train = x_train.transpose(0, 2, 3, 1)\n","    x_test = x_test.transpose(0, 2, 3, 1)\n","\n","  return (x_train, y_train), (x_test, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EQC9BguoQcfp","colab":{}},"source":["(x_train, y_train), (x_test, y_test) = load_data()\n","len_train, len_test = len(x_train), len(x_test)\n","y_train = y_train.astype('int64').reshape(len_train)\n","y_test = y_test.astype('int64').reshape(len_test)\n","\n","train_mean = np.mean(x_train, axis=(0,1,2))\n","train_std = np.std(x_train, axis=(0,1,2))\n","\n","normalize = lambda x: ((x - train_mean) / train_std).astype('float32') # todo: check here\n","pad4 = lambda x: np.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)], mode='reflect')\n","\n","#x_train = normalize(\n","x_train = pad4(x_train)\n","x_test = normalize(pad4(x_test))  ##PBO pad4 added"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DkR-X32Bomlb","outputId":"811b5455-5669-4c80-fab7-d4cd6a727d90","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["(x_train.shape, y_train.shape), (x_test.shape, y_test.shape)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(((50000, 40, 40, 3), (50000,)), ((10000, 40, 40, 3), (10000,)))"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FbzKiMardAzF","colab":{}},"source":["num_train, img_channels, img_rows, img_cols =  x_train.shape\n","# save the test dataset statistics\n","num_test, _, _, _ =  x_test.shape\n","# save the number of unique classes in the train dataset\n","num_classes = len(np.unique(y_train))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pOcjQdmt5WNX","colab_type":"text"},"source":["#### Initialize the dimension parameters"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eLe7Np1Ma48j","colab":{}},"source":["IMAGE_HEIGHT = 32 + 8\n","IMAGE_WIDTH = 32 + 8\n","IMAGE_DEPTH = 3\n","NUM_CLASSES = 10"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xsKBclV5bCV","colab_type":"text"},"source":["#### External code used to cutout the image"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RGd-2YmV8fZe","colab":{}},"source":["def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n","    def eraser(input_img):\n","        img_h, img_w, img_c = input_img.shape\n","        p_1 = np.random.rand()\n","\n","        if p_1 > p:\n","            return input_img\n","\n","        while True:\n","            s = np.random.uniform(s_l, s_h) * img_h * img_w\n","            r = np.random.uniform(r_1, r_2)\n","            w = int(np.sqrt(s / r))\n","            h = int(np.sqrt(s * r))\n","            left = np.random.randint(0, img_w)\n","            top = np.random.randint(0, img_h)\n","\n","            if left + w <= img_w and top + h <= img_h:\n","                break\n","\n","        if pixel_level:\n","            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n","        else:\n","            c = np.random.uniform(v_l, v_h)\n","\n","        input_img[top:top + h, left:left + w, :] = c\n","\n","        return input_img\n","\n","    return eraser"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fD2W1mtk5gUO","colab_type":"text"},"source":["#### Create the imagedatagenerator for train and test data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"X-YTh0eea451","colab":{}},"source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","\n","\n","# Image augmentation is done to increase the robustness of the model\n","datagen = ImageDataGenerator(zoom_range=0.0,\n","                             rotation_range=30 ,\n","                             featurewise_center=True,\n","                             featurewise_std_normalization=True,\n","                             horizontal_flip=True,\n","                             preprocessing_function=get_random_eraser(v_l=0, v_h=1))\n","\n","test_datagen = ImageDataGenerator(zoom_range=0.0,\n","                             featurewise_center=False,\n","                             featurewise_std_normalization=False,\n","                             horizontal_flip=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bV3OB40MJJ0z"},"source":["#### This code is used to create a folder to store the augmented images\n","dir_augmented_data = \"/root/.keras/augment\"\n","try:\n","  ##### if the preview folder does not exist, create\n","  os.mkdir(dir_augmented_data)\n","  \n","except:\n","  ##### if the preview folder exists, then remove\n","  ##### the contents (pictures) in the folder\n","  for item in os.listdir(dir_augmented_data):\n","    os.remove(dir_augmented_data + \"/\" + item)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bNg1Tv89coIc","colab":{}},"source":["#!ls /root/.keras/augment"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xGn7UKNbJSIo"},"source":["#### This code is used to augment the train image and save to directory\n","\n","##### Segregate the train data based on the class \n","##### Then for each record create 4 augmented images and save to directory\n","##### each file is prefixed with the class id , this information is used to create the label array for the images later\n","\n","class_ids = [0,1,2,3,4,5,6,7,8,9]\n","\n","for j in range(num_classes):\n","  idx = np.where(y_train[:]==j)[0]\n","  features_idx = x_train[idx,::]\n","  print(features_idx.shape)\n","  \n","  for i in range(len(features_idx)):\n","    \n","    i = 0\n","    Nplot = 4\n","    x = img_to_array(features_idx[i])\n","    ## reshape (Sampke, Nrow, Ncol, 3) 3 = R, G or B\n","    x = x.reshape((1,) + x.shape)\n","    for batch in datagen.flow(x ,batch_size=32,\n","                          save_to_dir=dir_augmented_data,\n","                          save_prefix= class_ids[j], # \"0\",\n","                          save_format='png'):\n","      i += 1\n","      \n","      if i > Nplot - 1: ## generate 8 pictures \n","        break    \n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BPA1ibA5a4zd","outputId":"08a6a81c-2a97-4291-99a0-2fa0c46b6fb8","colab":{}},"source":["!ls -la    /output/.keras/augment/tfrecord\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["total 45136\r\n","drwxr-xr-x 2 root root     4096 Aug 26 01:33 .\r\n","drwxr-xr-x 3 root root     4096 Aug 26 00:51 ..\r\n","-rw-r--r-- 1 root root 46210080 Aug 26 01:33 test.tfrecords-000\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mEi2uf0jhRdk","colab":{}},"source":["#!du -h /root/.keras/augment"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VMVpv8ZthjFD","colab":{}},"source":["#!rm -rf /output/.keras/augment/tfrecord/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bLkYNUVq6k8O","colab_type":"text"},"source":["##### 2 folders are created to save the augmented image and tfrecord"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hnNZQElD3kkp","outputId":"f9bc0158-570f-47e5-be25-fed66ce80b55","colab":{}},"source":["!mkdir /output/.keras/augment/\n","!mkdir /output/.keras/augment/tfrecord/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘/output/.keras/augment/’: File exists\n","mkdir: cannot create directory ‘/output/.keras/augment/tfrecord/’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mlTjTcsWE6Sk","colab":{}},"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rKUklo2kE6HW","colab":{}},"source":["#!cp  \"/content/drive/My Drive/Colab Notebooks/eva/Session14/tfrecord/test.tfrecords-000\" /root/.keras/augment/tfrecord/\n","#!cp  \"/content/drive/My Drive/Colab Notebooks/eva/Session14/tfrecord/train.tfrecords-000\" /root/.keras/augment/tfrecord/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O-8F4FUV7DXa","colab_type":"text"},"source":["##### If the tfrecord is created then the same is copied to a local folder"]},{"cell_type":"code","metadata":{"id":"kFLnI7ZDsZpe","colab_type":"code","outputId":"9ff877ce-0ca9-422d-e4a9-5851cf0c825e","colab":{}},"source":["!cp  \"/output/test.tfrecords-000\" /output/.keras/augment/tfrecord/\n","!cp  \"/output/test.tfrecords-000\" /root/.keras/augment/tfrecord/\n","#!cp  \"/content/drive/My Drive/Colab Notebooks/eva/Session14/tfrecord/train.tfrecords-000\" /root/.keras/augment/tfrecord//train.tfrecords-000\" /root/.keras/augment/tfrecord/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["cp: cannot create regular file '/root/.keras/augment/tfrecord/': No such file or directory\r\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FwBrSoqQJb90"},"source":["#### This code is used to create the tfrecord writer handle\n","\n","tfrecord file path\n","\n","TF_recordPath = \"/output/.keras/augment/tfrecord/\"\n","##### the best number of images stored in each tfrecord file\n","bestNum = 1000\n","##### the index of images flowing into each tfrecord file\n","num = 0\n","##### the index of the tfrecord file\n","recordFileNum = 0\n","\n","\"\"\"\n","##### name  of the tfrecord files\n","recordFileName = (\"train.tfrecords-%.3d\" % recordFileNum)\n","recordFileName_test = (\"test.tfrecords-%.3d\" % recordFileNum)\n","\n","##### tfrecord file writer\n","writer = tf.io.TFRecordWriter(TF_recordPath + recordFileName)\n","\n","writer_test = tf.io.TFRecordWriter(TF_recordPath + recordFileName_test)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Dq8L0aCVa4vh","colab":{}},"source":["#  tfrecord file path\n","\n","TF_recordPath = \"/output/.keras/augment/tfrecord/\"\n","# the best number of images stored in each tfrecord file\n","bestNum = 1000\n","# the index of images flowing into each tfrecord file\n","num = 0\n","# the index of the tfrecord file\n","recordFileNum = 0\n","\n","recordFileName = (\"train.tfrecords-%.3d\" % recordFileNum)\n","recordFileName_test = (\"test.tfrecords-%.3d\" % recordFileNum)\n","\n","\"\"\"\n","# name  of the tfrecord files\n","\n","# tfrecord file writer\n","writer = tf.io.TFRecordWriter(TF_recordPath + recordFileName)\n","\n","\n","\n","writer_test = tf.io.TFRecordWriter(TF_recordPath + recordFileName_test)\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tkVtxg2x7zVQ","colab_type":"text"},"source":["##### int64 and bytes wrapper"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"az8KEZE4rH59","colab":{}},"source":["def _int64_feature(value):\n","  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9zifhFE7rGez","colab":{}},"source":["def _bytes_feature(value):\n","  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[(value)]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1dFdkzB88CSP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LBXHkLatJhxo"},"source":["#### This code is used to create the tfrecord from the augmented images\n","\n","\n","output_file = TF_recordPath + recordFileName\n","with tf.python_io.TFRecordWriter(output_file) as record_writer:\n","  for infile in glob.glob(\"/root/.keras/augment/*.png\"): \n","    \n","    index1=infile.find('augment/') + 8\n","    TF_label = int(infile[index1:index1 +1])\n","    im = Image.open(infile)\n","    img_raw = im.tobytes() \n","    example = tf.train.Example(features=tf.train.Features(\n","          feature={\n","            'image': _bytes_feature(img_raw),\n","            'label': _int64_feature(TF_label)\n","          }))     \n","    record_writer.write(example.SerializeToString())"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TCGHkEJoJoiv"},"source":["\n","#### This code is used to save the test images into a local folder\n","\n","##### There is no augmentation done on the test data\n","\n","dir_test_data='/root/.keras/test'\n","os.mkdir(dir_test_data)\n","for j in range(num_classes):\n","  idx = np.where(y_test[:]==j)[0]\n","  features_idx_test = x_test[idx,::]\n","  print(features_idx_test.shape)\n","  \n","  for i in range(len(features_idx_test)):\n","    i = 0\n","    Nplot = 1\n","    x = img_to_array(features_idx_test[i])\n","    ## reshape (Sampke, Nrow, Ncol, 3) 3 = R, G or B\n","    x = x.reshape((1,) + x.shape)\n","    for batch in test_datagen.flow(x ,batch_size=32,\n","                          save_to_dir=dir_test_data,\n","                          save_prefix= class_ids[j], # \"0\",\n","                          save_format='png'):\n","      i += 1\n","      \n","      if i > Nplot - 1: ## generate 8 pictures \n","        break   "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"StWajZmTJtN8"},"source":["#### This code is used to create tfrecord for the test data\n","\n","output_file_test = TF_recordPath + recordFileName_test\n","with tf.python_io.TFRecordWriter(output_file_test) as record_writer:\n","  for infile in glob.glob(\"/root/.keras/test/*.png\"):\n","  \n","    index1=infile.find('test/') + 5\n","    TF_label = int(infile[index1:index1 +1])\n","    im = Image.open(infile)\n","    img_raw = im.tobytes() \n","    example = tf.train.Example(features=tf.train.Features(\n","          feature={\n","            'image': _bytes_feature(img_raw),\n","            'label': _int64_feature(TF_label)\n","          }))     \n","    record_writer.write(example.SerializeToString())"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nGmA_MwkJwWr"},"source":["\"\"\"print(\"Creating the single tfrecord file\")  \n","\n","for infile in glob.glob(\"/root/.keras/augment/*.png\"):\n","  index1=infile.find('augment/') + 8\n","  TF_label = int(infile[index1:index1 +1])\n","  im = Image.open(infile)\n","  img_raw = im.tobytes()\n","  example = tf.train.Example(features=tf.train.Features(feature={\n","            \"img_raw\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),\n","            \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[TF_label]))}))\n","\n","  writer.write(example.SerializeToString())\"\"\""]},{"cell_type":"markdown","metadata":{"id":"xFKfRkAI-BQX","colab_type":"text"},"source":["#### Create file path for train and test data tfrecord"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"G7adjijUa4oh","colab":{}},"source":["tfrecord_location = TF_recordPath\n","name = recordFileName\n","filename = os.path.join(tfrecord_location, name)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lQkpFx7xzRXu","colab":{}},"source":["name_test = recordFileName_test\n","filename_test = os.path.join(tfrecord_location, name_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6rLEmQATCTzK","outputId":"17fd9f98-9d0e-4fd9-9fcd-015acee9a80a","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!ls /output/.keras/augment/tfrecord/"],"execution_count":0,"outputs":[{"output_type":"stream","text":["test.tfrecords-000\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9XYnnkBzDeuk","outputId":"8d0a1e72-41bd-4032-bbd1-2135ae6e0562","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\"\"\"from google.colab import drive\n","drive.mount('/content/drive')\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"from google.colab import drive\\ndrive.mount('/content/drive')\""]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tc9TY6DfDzja","colab":{}},"source":["#!cp /root/.keras/augment/tfrecord/* \"/content/drive/My Drive/Colab Notebooks/eva/Session14/tfrecord\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GOlbyeJYEkGA","colab":{}},"source":["#!ls \"/content/drive/My Drive/Colab Notebooks/eva/Session14/tfrecord\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1lP8Wi_G-RX7","colab_type":"text"},"source":["#### Create the train and test dataset from the saved tfrecord"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"452dc-Zma4lm","colab":{}},"source":["dataset = tf.data.TFRecordDataset(filename)\n","dataset_test = tf.data.TFRecordDataset(filename_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vAbPtCd5yLUc","outputId":"39fd9c51-ef96-42f4-9799-578f04cf481e","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["type(dataset)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensorflow.python.data.ops.readers.TFRecordDataset"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"9YJU2Tev-ZuN","colab_type":"text"},"source":["#### External source code used to parse each record and convert to image tensor and label tensor"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eQcd_8vqa4hY","colab":{}},"source":["def parse_record(serialized_example):\n","  features = tf.parse_single_example(\n","    serialized_example,\n","    features={\n","      'image': tf.FixedLenFeature([], tf.string),\n","      'label': tf.FixedLenFeature([], tf.int64),\n","    })\n","  \n","  image = tf.decode_raw(features['image'], tf.uint8)\n","  image.set_shape([IMAGE_DEPTH * IMAGE_HEIGHT * IMAGE_WIDTH])\n","  image = tf.reshape(image, [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])\n","  image = tf.cast(tf.transpose(image, [1, 2, 0]), tf.float32)\n","  image = tf.random_crop(image,[ IMAGE_HEIGHT - 8 , IMAGE_WIDTH - 8 , IMAGE_DEPTH ])\n","  \n","  label = tf.cast(features['label'], tf.int64)\n","  #label = tf.one_hot(label, NUM_CLASSES)\n","\n","  return image, label"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S6lBcnF--s1H","colab_type":"text"},"source":["#### preprocessing ahead of time\n","\n","##### shuffle the tran dataset\n","##### parse the train and test dataset\n","##### preset the batch size in both dataset\n","##### prefetch 1 batch of train dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hVkZhpOBa4dp","colab":{}},"source":["buffer_size = BATCH_SIZE * 2 + 1\n","dataset = dataset.shuffle(buffer_size=buffer_size)\n","\n","dataset = dataset.map(parse_record ) #, num_parallel_calls= 10)\n","\n","#dataset_test = dataset_test.shuffle(buffer_size=buffer_size)\n","dataset_test = dataset_test.map(parse_record )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uXTENmc3a4aX","colab":{}},"source":["dataset = dataset.batch(BATCH_SIZE)\n","dataset = dataset.prefetch(1)\n","\n","dataset_test = dataset_test.batch(BATCH_SIZE)\n","dataset = dataset_test.prefetch(1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uYJuzjPd_lyw","colab_type":"text"},"source":["#### Create the model and define the lr schedule"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2-rBodFhQeNZ","colab":{}},"source":["model = DavidNet()\n","batches_per_epoch = len_train//BATCH_SIZE + 1\n","\n","schedule = np.interp(np.arange(EPOCHS+1), \n","                     [0, int((EPOCHS+1)*0.2), int((EPOCHS+1)*0.7), EPOCHS], \n","                     [LEARNING_RATE/5.0, LEARNING_RATE, LEARNING_RATE/5.0, 0.04])\n","\n","\n","lr_schedule = lambda t: schedule[int(t)]  #int cast PBO\n","\n","\n","#lr_schedule = lambda t: np.interp([t], [0, (EPOCHS+1)//5, EPOCHS], [0, LEARNING_RATE, 0])[0]  #PBO commented\n","\n","global_step = tf.train.get_or_create_global_step() ## uncommented PBO\n","lr_func = lambda: lr_schedule(global_step/batches_per_epoch)/BATCH_SIZE\n","opt = tf.train.MomentumOptimizer(lr_func, momentum=MOMENTUM, use_nesterov=True)\n","\n","##the below is replaced with preprocessing \n","def data_aug(x, y):\n","    return (tf.image.random_flip_left_right(tf.random_crop(x, [32, 32, 3])), y)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rid8Fg29_0pE","colab_type":"text"},"source":["#### Define a loss function "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pZjsiGufulyC","colab":{}},"source":["def lossfn(model , x , y):\n","  loss , _ = model(x , y)\n","  \n","  return loss\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3nRBltE3Wiok","colab":{}},"source":["#oss_scale_manager = tf.contrib.mixed_precision.FixedLossScaleManager(9000000)\n","\n","# Wraps the original optimizer in a LossScaleOptimizer.\n","#opt = tf.contrib.mixed_precision.LossScaleOptimizer(opt, loss_scale_manager)\n","\n","#opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)  ##PBO\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6IkxXLlGlAKB","outputId":"47d7bd00-bbd2-4fda-e123-c5274ff1992e","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["type(dataset)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensorflow.python.data.ops.dataset_ops.PrefetchDataset"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"VMoMuLjUAIrA","colab_type":"text"},"source":["#### Run the model for 3 epochs \n","\n","##### lr = 0.4\n","##### momentum = 0.9\n","##### batch size 512"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4f2NQh5N3_gQ","outputId":"cb6bdf34-0b0c-4643-a596-bedb528b2bc7","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":165}},"source":["t = time.time()\n","#test_set = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n","\n","i = 0\n","localepochs = EPOCHS\n","for epoch in range(localepochs):\n","  \n","  iterator = dataset.make_one_shot_iterator()\n","  #image_batch, label_batch = iterator.get_next()\n","  iterator_test = dataset_test.make_one_shot_iterator()\n","  train_loss = test_loss = train_acc = test_acc = 0.0\n","  #train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(data_aug, num_parallel_calls=20).shuffle(len_train).batch(BATCH_SIZE).prefetch(1)\n","  #train_set = dataset\n","  tf.keras.backend.set_learning_phase(1)\n","\n","  for (x, y) in iterator : # tqdm(train_set):\n","    gvs = tfe.implicit_gradients(lossfn)(model, x, y)\n","    #print(i)\n","    i += 1\n","    if gvs[0] is None:\n","      raise ValueError(\"No variables provided.\")\n","    applyGrad = True\n","    for g, v in gvs : # zip(grads, var):\n","      if (g is None):\n","        applyGrad = False\n","      else :\n","        g += v * WEIGHT_DECAY * BATCH_SIZE\n","      \n","    if (applyGrad):\n","      opt.apply_gradients( gvs, global_step= global_step ) #list(zip(grads, var)) tf.train.get_or_create_global_step()\n","    \n","  tf.keras.backend.set_learning_phase(0)\n","  \n","  #if (((epoch + 1 == 1) or (epoch + 1 == localepochs ))):\n","  \n","  for (x, y) in iterator_test : #test_set:\n","    loss, correct = model(x, y)\n","    #print(loss.numpy())\n","    test_loss += loss.numpy()\n","    test_acc += correct.numpy()\n","    \n","  print('epoch:', epoch+1, 'lr:', lr_schedule(epoch+1), 'train loss:', train_loss / len_train, 'train acc:', train_acc / len_train, 'val loss:', test_loss / len_test, 'val acc:', test_acc / len_test, 'time:', time.time() - t)    \n","    \n"," #   print('epoch:', epoch+1, 'lr:', lr_schedule(epoch+1), 'train loss:', train_loss / len_train, 'train acc:', train_acc / len_train, 'val loss:', test_loss / len_test, 'val acc:', test_acc / len_test, 'time:', time.time() - t)\n","      \n","      \n","  with open('MODELACCY1.txt' , 'a') as f:\n","      f.write( str(epoch+1))\n","      f.write(\"\\t\")\n","      f.write(str(time.time() - t))\n","      f.write(\"\\t\")\n","      f.write(str(lr_schedule(epoch+1))) \n","      f.write(\"\\t\")\n","      f.write( str(test_loss / len_test))\n","      f.write(\"\\t\")\n","      f.write( str(test_acc / len_test))\n","      \n","      f.write(\"\\n\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch: 1 lr: 0.24000000000000002 train loss: 0.0 train acc: 0.0 val loss: 46.76940234375 val acc: 0.0957 time: 14.404902458190918\n","epoch: 2 lr: 0.08 train loss: 0.0 train acc: 0.0 val loss: 0.5401712661743164 val acc: 0.6829 time: 22.11995816230774\n","epoch: 3 lr: 0.04 train loss: 0.0 train acc: 0.0 val loss: 0.0012118663668632507 val acc: 0.952 time: 29.8717782497406\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UPaTpX60AbJ2","colab_type":"text"},"source":["## Achieved 95.2% accuracy in 3 epoch\n","\n","####epoch: 3 lr: 0.04 train loss: 0.0 train acc: 0.0 val loss: 0.0012118663668632507 val acc: 0.952 time: 29.8717782497406"]}]}